{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"battle\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 3 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 3 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 409600 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 144 # Number of units in hidden layer.\n",
    "batch_size = 5120 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: KnightBrainInternal\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 65\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000. Mean Reward: -0.17115474925513569. Std of Reward: 1.0139283161261667.\n",
      "Step: 20000. Mean Reward: -0.25322569781806187. Std of Reward: 0.9550361846666271.\n",
      "Step: 30000. Mean Reward: -0.2901227363312768. Std of Reward: 0.9413936064340942.\n",
      "Step: 40000. Mean Reward: -0.20769890631691992. Std of Reward: 0.9405057222343218.\n",
      "Step: 50000. Mean Reward: -0.27438938029084875. Std of Reward: 0.9582975460084912.\n",
      "Saved Model\n",
      "Step: 60000. Mean Reward: -0.2797139664099594. Std of Reward: 0.9638809953601756.\n",
      "Step: 70000. Mean Reward: -0.2427496628812414. Std of Reward: 0.9553797958940302.\n",
      "Step: 80000. Mean Reward: -0.26756178940257613. Std of Reward: 0.9435341231294514.\n",
      "Step: 90000. Mean Reward: -0.2082782162460822. Std of Reward: 0.9489871108602671.\n",
      "Step: 100000. Mean Reward: -0.2671558042308666. Std of Reward: 0.9555291689706891.\n",
      "Saved Model\n",
      "Step: 110000. Mean Reward: -0.3264387184895881. Std of Reward: 0.947482308629605.\n",
      "Step: 120000. Mean Reward: -0.31377795324864344. Std of Reward: 0.9530335587567115.\n",
      "Step: 130000. Mean Reward: -0.3180685474152814. Std of Reward: 0.9378831132864778.\n",
      "Step: 140000. Mean Reward: -0.28472670342462125. Std of Reward: 0.9454946278284528.\n",
      "Step: 150000. Mean Reward: -0.36433643154392475. Std of Reward: 0.9055745012791302.\n",
      "Saved Model\n",
      "Step: 160000. Mean Reward: -0.337930876071232. Std of Reward: 0.9144963036568187.\n",
      "Step: 170000. Mean Reward: -0.35936361842507203. Std of Reward: 0.918544852682943.\n",
      "Step: 180000. Mean Reward: -0.36916381553064587. Std of Reward: 0.8924128837973098.\n",
      "Step: 190000. Mean Reward: -0.33562901549836516. Std of Reward: 0.9271427922366536.\n",
      "Step: 200000. Mean Reward: -0.3130965326913178. Std of Reward: 0.9454659131838643.\n",
      "Saved Model\n",
      "Step: 210000. Mean Reward: -0.3857804218278165. Std of Reward: 0.909298804446461.\n",
      "Step: 220000. Mean Reward: -0.27919094697739205. Std of Reward: 0.9748672096941651.\n",
      "Step: 230000. Mean Reward: -0.4247147557611445. Std of Reward: 0.8890547302980726.\n",
      "Step: 240000. Mean Reward: -0.41972559281122884. Std of Reward: 0.8925053741237106.\n",
      "Step: 250000. Mean Reward: -0.4347069882566601. Std of Reward: 0.8789707932554358.\n",
      "Saved Model\n",
      "Step: 260000. Mean Reward: -0.33955041158119564. Std of Reward: 0.9452960411245155.\n",
      "Step: 270000. Mean Reward: -0.4357521431843622. Std of Reward: 0.8781789671786694.\n",
      "Step: 280000. Mean Reward: -0.3355415462573205. Std of Reward: 0.9606567788653623.\n",
      "Step: 290000. Mean Reward: -0.31767343211961496. Std of Reward: 0.9867015998485782.\n",
      "Step: 300000. Mean Reward: -0.3608929711661082. Std of Reward: 0.926077274008352.\n",
      "Saved Model\n",
      "Step: 310000. Mean Reward: -0.3735347014735006. Std of Reward: 0.9485204677387831.\n",
      "Step: 320000. Mean Reward: -0.3831624613484051. Std of Reward: 0.9049291622775958.\n",
      "Step: 330000. Mean Reward: -0.37936959736703385. Std of Reward: 0.9071787560447316.\n",
      "Step: 340000. Mean Reward: -0.3542375920204625. Std of Reward: 0.936604586490543.\n",
      "Step: 350000. Mean Reward: -0.35796170806197347. Std of Reward: 0.9413995860900224.\n",
      "Saved Model\n",
      "Step: 360000. Mean Reward: -0.24799758147398443. Std of Reward: 0.959322682264154.\n",
      "Step: 370000. Mean Reward: -0.22006798138236297. Std of Reward: 0.9606953928205315.\n",
      "Step: 380000. Mean Reward: -0.2689377573666853. Std of Reward: 0.9399819278312035.\n",
      "Step: 390000. Mean Reward: -0.29968507983148573. Std of Reward: 0.9344411938731975.\n",
      "Step: 400000. Mean Reward: -0.3191371704254617. Std of Reward: 0.9532100643284325.\n",
      "Saved Model\n",
      "Step: 410000. Mean Reward: -0.3239089149356326. Std of Reward: 0.9531957856243575.\n",
      "Step: 420000. Mean Reward: -0.2609883571224844. Std of Reward: 0.9926816934480346.\n",
      "Step: 430000. Mean Reward: -0.28236595540852527. Std of Reward: 0.9625674667623808.\n",
      "Step: 440000. Mean Reward: -0.27486476005352006. Std of Reward: 0.9251247718791243.\n",
      "Step: 450000. Mean Reward: -0.32645122478260463. Std of Reward: 0.9541864721349901.\n",
      "Saved Model\n",
      "Step: 460000. Mean Reward: -0.29263276848796643. Std of Reward: 0.933194319211842.\n",
      "Step: 470000. Mean Reward: -0.30083707254845377. Std of Reward: 0.9458166734928071.\n",
      "Step: 480000. Mean Reward: -0.1555829806649679. Std of Reward: 0.976789051138011.\n",
      "Step: 490000. Mean Reward: -0.20643906583232854. Std of Reward: 0.9943951819329869.\n",
      "Step: 500000. Mean Reward: -0.2703987903960796. Std of Reward: 0.9633866307205924.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: -0.18977729462214657. Std of Reward: 0.9602798494451992.\n",
      "Step: 520000. Mean Reward: -0.2420119447779523. Std of Reward: 0.9594950773280916.\n",
      "Step: 530000. Mean Reward: -0.18026051748085434. Std of Reward: 0.9902272602993061.\n",
      "Step: 540000. Mean Reward: -0.22451406713466984. Std of Reward: 0.9672253378732061.\n",
      "Step: 550000. Mean Reward: -0.223911027583005. Std of Reward: 0.9592185392201301.\n",
      "Saved Model\n",
      "Step: 560000. Mean Reward: -0.2667362196451015. Std of Reward: 0.9763109177709681.\n",
      "Step: 570000. Mean Reward: -0.19471122769862115. Std of Reward: 0.9805701939215615.\n",
      "Step: 580000. Mean Reward: -0.2973383332478738. Std of Reward: 0.9408596407187075.\n",
      "Step: 590000. Mean Reward: -0.2224499573757032. Std of Reward: 0.9608493916444849.\n",
      "Step: 600000. Mean Reward: -0.2436462413323807. Std of Reward: 0.9648210295384555.\n",
      "Saved Model\n",
      "Step: 610000. Mean Reward: -0.28194651166578477. Std of Reward: 0.9282431170019119.\n",
      "Step: 620000. Mean Reward: -0.15385537349940334. Std of Reward: 0.946358719023159.\n",
      "Step: 630000. Mean Reward: -0.16976417877081515. Std of Reward: 0.9632734662449443.\n",
      "Step: 640000. Mean Reward: -0.25966338906556946. Std of Reward: 0.9145415809435737.\n",
      "Step: 650000. Mean Reward: -0.19480340596962048. Std of Reward: 0.9492233243112507.\n",
      "Saved Model\n",
      "Step: 660000. Mean Reward: -0.2206272060819977. Std of Reward: 0.9818417391348994.\n",
      "Step: 670000. Mean Reward: -0.1542014102094839. Std of Reward: 0.9642126668503952.\n",
      "Step: 680000. Mean Reward: -0.15753369682186666. Std of Reward: 1.0059324965239322.\n",
      "Step: 690000. Mean Reward: -0.1683479379250757. Std of Reward: 0.9714847658432731.\n",
      "Step: 700000. Mean Reward: -0.25245681766485256. Std of Reward: 0.9767144512063131.\n",
      "Saved Model\n",
      "Step: 710000. Mean Reward: -0.19763109729886502. Std of Reward: 0.9433544203721985.\n",
      "Step: 720000. Mean Reward: -0.19905000988212823. Std of Reward: 0.9538043645203371.\n",
      "Step: 730000. Mean Reward: -0.1925819880771374. Std of Reward: 0.9920188452417862.\n",
      "Step: 740000. Mean Reward: -0.2447584593787707. Std of Reward: 0.9604597588892136.\n",
      "Step: 750000. Mean Reward: -0.19959587731740946. Std of Reward: 1.0015145072415774.\n",
      "Saved Model\n",
      "Step: 760000. Mean Reward: -0.19797411043943242. Std of Reward: 0.9907800101627123.\n",
      "Step: 770000. Mean Reward: -0.23493220158831177. Std of Reward: 0.9619548531655167.\n",
      "Step: 780000. Mean Reward: -0.2164556965512475. Std of Reward: 0.9713341626481641.\n",
      "Step: 790000. Mean Reward: -0.20900501666712304. Std of Reward: 0.9646041907245013.\n",
      "Step: 800000. Mean Reward: -0.2433960859119301. Std of Reward: 0.9375529363273593.\n",
      "Saved Model\n",
      "Step: 810000. Mean Reward: -0.1890841969644669. Std of Reward: 0.993982679058422.\n",
      "Step: 820000. Mean Reward: -0.2292203989158304. Std of Reward: 0.9655128607001353.\n",
      "Step: 830000. Mean Reward: -0.2015902947594964. Std of Reward: 0.9674407667650246.\n",
      "Step: 840000. Mean Reward: -0.22532706137219852. Std of Reward: 0.9764110421909875.\n",
      "Step: 850000. Mean Reward: -0.1909275077843718. Std of Reward: 0.9538676767592312.\n",
      "Saved Model\n",
      "Step: 860000. Mean Reward: -0.21268040731247476. Std of Reward: 0.9332134447111149.\n",
      "Step: 870000. Mean Reward: -0.22726746264887285. Std of Reward: 0.9659470891174893.\n",
      "Step: 880000. Mean Reward: -0.21151683864060622. Std of Reward: 0.9617551475500278.\n",
      "Step: 890000. Mean Reward: -0.23167888730496689. Std of Reward: 0.9933781561980037.\n",
      "Step: 900000. Mean Reward: -0.17232918822006485. Std of Reward: 0.984089905169015.\n",
      "Saved Model\n",
      "Step: 910000. Mean Reward: -0.16129160972005488. Std of Reward: 0.9923373529302697.\n",
      "Step: 920000. Mean Reward: -0.29965518733174523. Std of Reward: 0.8906382706475666.\n",
      "Step: 930000. Mean Reward: -0.24025128132928125. Std of Reward: 0.9765574239734061.\n",
      "Step: 940000. Mean Reward: -0.24104277746010552. Std of Reward: 0.9234168707649842.\n",
      "Step: 950000. Mean Reward: -0.2199987791114582. Std of Reward: 0.980446587671252.\n",
      "Saved Model\n",
      "Step: 960000. Mean Reward: -0.25104626218682696. Std of Reward: 0.9750060227428472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 970000. Mean Reward: -0.31291194987060217. Std of Reward: 0.9596853044699495.\n",
      "Step: 980000. Mean Reward: -0.259530083741309. Std of Reward: 0.9423633101981558.\n",
      "Step: 990000. Mean Reward: -0.2602942827680722. Std of Reward: 0.9837828440973534.\n",
      "Step: 1000000. Mean Reward: -0.3272717389349491. Std of Reward: 0.9417112696750287.\n",
      "Saved Model\n",
      "Step: 1010000. Mean Reward: -0.34142708900418633. Std of Reward: 0.9191398749144024.\n",
      "Step: 1020000. Mean Reward: -0.2161701105930068. Std of Reward: 0.9902734734123567.\n",
      "Step: 1030000. Mean Reward: -0.24866390146057712. Std of Reward: 0.9344014713869052.\n",
      "Step: 1040000. Mean Reward: -0.21691872853843075. Std of Reward: 0.9793290862633436.\n",
      "Step: 1050000. Mean Reward: -0.2328894846000743. Std of Reward: 0.9753244486447905.\n",
      "Saved Model\n",
      "Step: 1060000. Mean Reward: -0.2775029246767759. Std of Reward: 0.9358666107662574.\n",
      "Step: 1070000. Mean Reward: -0.2200089487812437. Std of Reward: 0.9699868567663248.\n",
      "Step: 1080000. Mean Reward: -0.2696378519593814. Std of Reward: 0.956091071161895.\n",
      "Step: 1090000. Mean Reward: -0.20338260869994138. Std of Reward: 0.9658827370353367.\n",
      "Step: 1100000. Mean Reward: -0.19357985282216417. Std of Reward: 0.97436705966548.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: -0.24825563944635667. Std of Reward: 0.9608408616948213.\n",
      "Step: 1120000. Mean Reward: -0.25049822081662315. Std of Reward: 0.9460402122043426.\n",
      "Step: 1130000. Mean Reward: -0.26563454765558603. Std of Reward: 0.9625802234678702.\n",
      "Step: 1140000. Mean Reward: -0.1845354594803392. Std of Reward: 0.9840207554676654.\n",
      "Step: 1150000. Mean Reward: -0.15309625097140286. Std of Reward: 1.0022909690877315.\n",
      "Saved Model\n",
      "Step: 1160000. Mean Reward: -0.2707385532409921. Std of Reward: 0.9929158298587082.\n",
      "Step: 1170000. Mean Reward: -0.1949519032142978. Std of Reward: 0.9732098026606246.\n",
      "Step: 1180000. Mean Reward: -0.24395484629292036. Std of Reward: 0.9990995895261755.\n",
      "Step: 1190000. Mean Reward: -0.3320776824964057. Std of Reward: 0.9244072212161915.\n",
      "Step: 1200000. Mean Reward: -0.3724964318438354. Std of Reward: 0.9130159327858186.\n",
      "Saved Model\n",
      "Step: 1210000. Mean Reward: -0.3254213397397238. Std of Reward: 0.9391428521591036.\n",
      "Step: 1220000. Mean Reward: -0.34832078883144785. Std of Reward: 0.9205158744223366.\n",
      "Step: 1230000. Mean Reward: -0.23736711407136135. Std of Reward: 0.981476160940395.\n",
      "Step: 1240000. Mean Reward: -0.23718657422410946. Std of Reward: 0.9736879268048532.\n",
      "Step: 1250000. Mean Reward: -0.2094917868732715. Std of Reward: 1.0049632244608815.\n",
      "Saved Model\n",
      "Step: 1260000. Mean Reward: -0.3216017891563561. Std of Reward: 0.9565256097947512.\n",
      "Step: 1270000. Mean Reward: -0.2922070445323075. Std of Reward: 0.924298862299015.\n",
      "Step: 1280000. Mean Reward: -0.3010490824443785. Std of Reward: 0.9515228260501944.\n",
      "Step: 1290000. Mean Reward: -0.2638159098670869. Std of Reward: 0.9439020870980644.\n",
      "Step: 1300000. Mean Reward: -0.2594164768464594. Std of Reward: 0.9361567369438879.\n",
      "Saved Model\n",
      "Step: 1310000. Mean Reward: -0.2689885913646565. Std of Reward: 0.967046479169363.\n",
      "Step: 1320000. Mean Reward: -0.27631369152138396. Std of Reward: 0.9130310637657355.\n",
      "Step: 1330000. Mean Reward: -0.2926174891776871. Std of Reward: 0.9784584080606793.\n",
      "Step: 1340000. Mean Reward: -0.2606811673869579. Std of Reward: 0.9739098707316814.\n",
      "Step: 1350000. Mean Reward: -0.26702431793119125. Std of Reward: 0.9826405950224099.\n",
      "Saved Model\n",
      "Step: 1360000. Mean Reward: -0.23007370267452507. Std of Reward: 0.9630074909329611.\n",
      "Step: 1370000. Mean Reward: -0.34042435639500923. Std of Reward: 0.9138511960472738.\n",
      "Step: 1380000. Mean Reward: -0.3715851888568485. Std of Reward: 0.9344352822118581.\n",
      "Step: 1390000. Mean Reward: -0.3416486258680608. Std of Reward: 0.9370278275539888.\n",
      "Step: 1400000. Mean Reward: -0.32125442623876505. Std of Reward: 0.9562318173293415.\n",
      "Saved Model\n",
      "Step: 1410000. Mean Reward: -0.3211450555148719. Std of Reward: 0.9047789423012937.\n",
      "Step: 1420000. Mean Reward: -0.2976048981474387. Std of Reward: 0.952988280976113.\n",
      "Step: 1430000. Mean Reward: -0.34963914131195284. Std of Reward: 0.9442555778769.\n",
      "Step: 1440000. Mean Reward: -0.3511991987642417. Std of Reward: 0.932576390141681.\n",
      "Step: 1450000. Mean Reward: -0.3549355006695807. Std of Reward: 0.9397300089539313.\n",
      "Saved Model\n",
      "Step: 1460000. Mean Reward: -0.3767942300773855. Std of Reward: 0.9128599863589614.\n",
      "Step: 1470000. Mean Reward: -0.3494405017257734. Std of Reward: 0.9702307819059822.\n",
      "Step: 1480000. Mean Reward: -0.3232828541713053. Std of Reward: 0.9340821865744824.\n",
      "Step: 1490000. Mean Reward: -0.4009036129829173. Std of Reward: 0.8858634454078792.\n",
      "Step: 1500000. Mean Reward: -0.29112727738962924. Std of Reward: 0.947410618773293.\n",
      "Saved Model\n",
      "Step: 1510000. Mean Reward: -0.3014201101694368. Std of Reward: 0.9608196454016229.\n",
      "Step: 1520000. Mean Reward: -0.3301297205194353. Std of Reward: 0.9310747273560229.\n",
      "Step: 1530000. Mean Reward: -0.2803235898105748. Std of Reward: 0.9337392660316756.\n",
      "Step: 1540000. Mean Reward: -0.28060684415171516. Std of Reward: 0.9814647443933465.\n",
      "Step: 1550000. Mean Reward: -0.2889919897580209. Std of Reward: 0.9524647866266769.\n",
      "Saved Model\n",
      "Step: 1560000. Mean Reward: -0.25354947171520725. Std of Reward: 0.9529881795412621.\n",
      "Step: 1570000. Mean Reward: -0.3375128461564899. Std of Reward: 0.9519851984562244.\n",
      "Step: 1580000. Mean Reward: -0.357124167131156. Std of Reward: 0.9018787360699806.\n",
      "Step: 1590000. Mean Reward: -0.29884588968624154. Std of Reward: 0.9412636545248608.\n",
      "Step: 1600000. Mean Reward: -0.35247129829994733. Std of Reward: 0.9694088830265907.\n",
      "Saved Model\n",
      "Step: 1610000. Mean Reward: -0.2918868471140305. Std of Reward: 0.9822627402495983.\n",
      "Step: 1620000. Mean Reward: -0.28162773793109136. Std of Reward: 0.9540742277982744.\n",
      "Step: 1630000. Mean Reward: -0.31351496651546196. Std of Reward: 0.9513796241256213.\n",
      "Step: 1640000. Mean Reward: -0.32171669168813183. Std of Reward: 0.935671003329536.\n",
      "Step: 1650000. Mean Reward: -0.26589661140460535. Std of Reward: 0.9888358738498862.\n",
      "Saved Model\n",
      "Step: 1660000. Mean Reward: -0.29610406049053806. Std of Reward: 0.9539525489723293.\n",
      "Step: 1670000. Mean Reward: -0.26012854628202986. Std of Reward: 0.9419556157462764.\n",
      "Step: 1680000. Mean Reward: -0.34100529400038637. Std of Reward: 0.9304819749850417.\n",
      "Step: 1690000. Mean Reward: -0.29714824754395275. Std of Reward: 0.9558421188276598.\n",
      "Step: 1700000. Mean Reward: -0.26147498740600006. Std of Reward: 0.9788759216636553.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: -0.3221620021321914. Std of Reward: 0.9376993738119493.\n",
      "Step: 1720000. Mean Reward: -0.29079669057927854. Std of Reward: 0.9597157232488557.\n",
      "Step: 1730000. Mean Reward: -0.24495103513824115. Std of Reward: 0.9668072353708373.\n",
      "Step: 1740000. Mean Reward: -0.2516991029342617. Std of Reward: 0.9674589837295028.\n",
      "Step: 1750000. Mean Reward: -0.25027819504797516. Std of Reward: 0.9629489216116403.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: -0.31032385852476996. Std of Reward: 0.9485376402124788.\n",
      "Step: 1770000. Mean Reward: -0.3519634822651446. Std of Reward: 0.9524001813177722.\n",
      "Step: 1780000. Mean Reward: -0.3077627818378162. Std of Reward: 0.9457547553151098.\n",
      "Step: 1790000. Mean Reward: -0.28928233972371853. Std of Reward: 0.9821157477049293.\n",
      "Step: 1800000. Mean Reward: -0.2912287624793242. Std of Reward: 0.974747322050843.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: -0.3133846845271228. Std of Reward: 0.9270431676995735.\n",
      "Step: 1820000. Mean Reward: -0.23506156112166304. Std of Reward: 0.9731779137246922.\n",
      "Step: 1830000. Mean Reward: -0.27967685471778875. Std of Reward: 0.9814056747537631.\n",
      "Step: 1840000. Mean Reward: -0.22180479514753684. Std of Reward: 0.9667517468233692.\n",
      "Step: 1850000. Mean Reward: -0.2615803094569188. Std of Reward: 0.9563536441242256.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: -0.36912558734735124. Std of Reward: 0.9165536207218539.\n",
      "Step: 1870000. Mean Reward: -0.3504584221184105. Std of Reward: 0.9189371415313026.\n",
      "Step: 1880000. Mean Reward: -0.30013551708717395. Std of Reward: 0.9798682624694549.\n",
      "Step: 1890000. Mean Reward: -0.23629381036374414. Std of Reward: 0.9791695625811999.\n",
      "Step: 1900000. Mean Reward: -0.36148258619861245. Std of Reward: 0.9323061678331199.\n",
      "Saved Model\n",
      "Step: 1910000. Mean Reward: -0.31411616288519295. Std of Reward: 0.9484707104326947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1920000. Mean Reward: -0.3022822747862964. Std of Reward: 0.9442688749250199.\n",
      "Step: 1930000. Mean Reward: -0.2928720059639936. Std of Reward: 0.9746607435787129.\n",
      "Step: 1940000. Mean Reward: -0.33652118346852194. Std of Reward: 0.9572949987455538.\n",
      "Step: 1950000. Mean Reward: -0.31397681383787224. Std of Reward: 0.9608217244565874.\n",
      "Saved Model\n",
      "Step: 1960000. Mean Reward: -0.326842550596852. Std of Reward: 0.9461011483583794.\n",
      "Step: 1970000. Mean Reward: -0.2427058338620126. Std of Reward: 0.9665598498767178.\n",
      "Step: 1980000. Mean Reward: -0.3427398201331913. Std of Reward: 0.9343503348811548.\n",
      "Step: 1990000. Mean Reward: -0.32752295422458366. Std of Reward: 0.9395202802394493.\n",
      "Step: 2000000. Mean Reward: -0.23497499042093437. Std of Reward: 0.9625157298658414.\n",
      "Saved Model\n",
      "Step: 2010000. Mean Reward: -0.29877142458449124. Std of Reward: 0.9547338662313738.\n",
      "Step: 2020000. Mean Reward: -0.30527542184604783. Std of Reward: 0.9399208319555316.\n",
      "Step: 2030000. Mean Reward: -0.3051687250062107. Std of Reward: 0.9394171190051176.\n",
      "Step: 2040000. Mean Reward: -0.36221453403905535. Std of Reward: 0.9265046646430584.\n",
      "Step: 2050000. Mean Reward: -0.28767998450054993. Std of Reward: 0.9729435959092917.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: -0.29529723064071667. Std of Reward: 0.9472597447433937.\n",
      "Step: 2070000. Mean Reward: -0.2667115541439931. Std of Reward: 0.9623546195422727.\n",
      "Step: 2080000. Mean Reward: -0.3174754504098321. Std of Reward: 0.9451903373845773.\n",
      "Step: 2090000. Mean Reward: -0.2787686604005534. Std of Reward: 0.9536236151393335.\n",
      "Step: 2100000. Mean Reward: -0.2822944893850511. Std of Reward: 0.9709056588494036.\n",
      "Saved Model\n",
      "Step: 2110000. Mean Reward: -0.33309745757442083. Std of Reward: 0.928191535732815.\n",
      "Step: 2120000. Mean Reward: -0.36519399017684634. Std of Reward: 0.9509943958139246.\n",
      "Step: 2130000. Mean Reward: -0.3462547240103762. Std of Reward: 0.9342522674336923.\n",
      "Step: 2140000. Mean Reward: -0.3412827237572211. Std of Reward: 0.9293274362639029.\n",
      "Step: 2150000. Mean Reward: -0.38890307806265584. Std of Reward: 0.9055444839446907.\n",
      "Saved Model\n",
      "Step: 2160000. Mean Reward: -0.3981599382036105. Std of Reward: 0.9215964539745934.\n",
      "Step: 2170000. Mean Reward: -0.361674991107603. Std of Reward: 0.9468728542010204.\n",
      "Step: 2180000. Mean Reward: -0.34263002608465826. Std of Reward: 0.9535635466462344.\n",
      "Step: 2190000. Mean Reward: -0.3178318948808958. Std of Reward: 0.9687349407971857.\n",
      "Step: 2200000. Mean Reward: -0.38802897327808483. Std of Reward: 0.9112160684762907.\n",
      "Saved Model\n",
      "Step: 2210000. Mean Reward: -0.28591575585390877. Std of Reward: 0.9840583227887234.\n",
      "Step: 2220000. Mean Reward: -0.34489187534746535. Std of Reward: 0.9209234366907698.\n",
      "Step: 2230000. Mean Reward: -0.37576014676025654. Std of Reward: 0.9205780656592639.\n",
      "Step: 2240000. Mean Reward: -0.3352466813156969. Std of Reward: 0.9523300808411234.\n",
      "Step: 2250000. Mean Reward: -0.2704259030576926. Std of Reward: 0.9501790022988965.\n",
      "Saved Model\n",
      "Step: 2260000. Mean Reward: -0.2959091097150356. Std of Reward: 0.9249693710635714.\n",
      "Step: 2270000. Mean Reward: -0.3733107365616584. Std of Reward: 0.9228011820527138.\n",
      "Step: 2280000. Mean Reward: -0.2933004731701206. Std of Reward: 0.9623510673211227.\n",
      "Step: 2290000. Mean Reward: -0.37163205653460957. Std of Reward: 0.9142631752723814.\n",
      "Step: 2300000. Mean Reward: -0.2953907459882231. Std of Reward: 0.9569310558581227.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: -0.2641019492125792. Std of Reward: 0.97894141328137.\n",
      "Step: 2320000. Mean Reward: -0.41539389942144533. Std of Reward: 0.9126252344010628.\n",
      "Step: 2330000. Mean Reward: -0.3240711997889482. Std of Reward: 0.9495948508992582.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-50001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-50001.cptk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0d967e19d86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexport_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/ml-agents/python/ppo/models.py\u001b[0m in \u001b[0;36mexport_graph\u001b[0;34m(model_path, env_name, target_nodes)\u001b[0m\n\u001b[1;32m     56\u001b[0m                               \u001b[0moutput_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.bytes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                               \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_saver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                               restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\")\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph\u001b[0;34m(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_whitelist, variable_names_blacklist, input_meta_graph, input_saved_model_dir, saved_model_tags)\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mrestore_op_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names_whitelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names_blacklist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       input_meta_graph_def, input_saved_model_dir, saved_model_tags.split(\",\"))\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph_with_def_protos\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mvar_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1666\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 run_metadata):\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1354\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1041\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1042\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/encoder.py\u001b[0m in \u001b[0;36mEncodeRepeatedField\u001b[0;34m(write, value, deterministic)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEncodeRepeatedField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value, deterministic)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0mvalue_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m       \u001b[0mentry_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m       \u001b[0mencode_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
