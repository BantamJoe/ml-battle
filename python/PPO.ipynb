{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"battle1\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 4 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 409600 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 # Number of units in hidden layer.\n",
    "batch_size = 5120 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: KnightBrainInternal\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 65\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-6000000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-6000000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 6010000. Mean Reward: 0.16079630521882052. Std of Reward: 0.8739746306772338.\n",
      "Step: 6020000. Mean Reward: 0.14588699178233794. Std of Reward: 0.9233279231520125.\n",
      "Step: 6030000. Mean Reward: 0.15530362023092947. Std of Reward: 0.9592444747800462.\n",
      "Step: 6040000. Mean Reward: 0.11697882976362385. Std of Reward: 0.9567132711368739.\n",
      "Step: 6050000. Mean Reward: 0.13955797011730867. Std of Reward: 0.9464953381694872.\n",
      "Saved Model\n",
      "Step: 6060000. Mean Reward: 0.11657153806726021. Std of Reward: 0.9445019402198195.\n",
      "Step: 6070000. Mean Reward: 0.13973794727642613. Std of Reward: 0.9353966526991485.\n",
      "Step: 6080000. Mean Reward: 0.16969541587038584. Std of Reward: 0.9383504840932547.\n",
      "Step: 6090000. Mean Reward: 0.22572973501245977. Std of Reward: 0.9457834137138451.\n",
      "Step: 6100000. Mean Reward: 0.19010421390634102. Std of Reward: 0.9338804642696767.\n",
      "Saved Model\n",
      "Step: 6110000. Mean Reward: 0.07625486080244485. Std of Reward: 0.9332467751424411.\n",
      "Step: 6120000. Mean Reward: 0.08198063667380433. Std of Reward: 0.9227747151084328.\n",
      "Step: 6130000. Mean Reward: 0.07255111798940063. Std of Reward: 0.9198172663064221.\n",
      "Step: 6140000. Mean Reward: 0.09665159451670055. Std of Reward: 0.9150982798801446.\n",
      "Step: 6150000. Mean Reward: 0.1545795928157912. Std of Reward: 0.9524263307538229.\n",
      "Saved Model\n",
      "Step: 6160000. Mean Reward: 0.21441112624845768. Std of Reward: 0.9046432598242338.\n",
      "Step: 6170000. Mean Reward: 0.3041333313057057. Std of Reward: 0.8972553911964359.\n",
      "Step: 6180000. Mean Reward: 0.38253441665605337. Std of Reward: 0.8576655203516141.\n",
      "Step: 6190000. Mean Reward: 0.3900650934262546. Std of Reward: 0.8838258624430178.\n",
      "Step: 6200000. Mean Reward: 0.45228342283996836. Std of Reward: 0.8653203653227575.\n",
      "Saved Model\n",
      "Step: 6210000. Mean Reward: 0.41363348969290686. Std of Reward: 0.8734464780608449.\n",
      "Step: 6220000. Mean Reward: 0.397586620409987. Std of Reward: 0.8898059766801171.\n",
      "Step: 6230000. Mean Reward: 0.39709544189767876. Std of Reward: 0.8962103581386969.\n",
      "Step: 6240000. Mean Reward: 0.41134016044975535. Std of Reward: 0.87071006442808.\n",
      "Step: 6250000. Mean Reward: 0.28961104955336825. Std of Reward: 0.8983385310375127.\n",
      "Saved Model\n",
      "Step: 6260000. Mean Reward: 0.19356707479857072. Std of Reward: 0.9038505397425912.\n",
      "Step: 6270000. Mean Reward: 0.12276516994363626. Std of Reward: 0.9134510294621069.\n",
      "Step: 6280000. Mean Reward: 0.12920578821724923. Std of Reward: 0.886814180684293.\n",
      "Step: 6290000. Mean Reward: 0.08500557164645148. Std of Reward: 0.9168584027758827.\n",
      "Step: 6300000. Mean Reward: 0.19262489696378185. Std of Reward: 0.910737029350378.\n",
      "Saved Model\n",
      "Step: 6310000. Mean Reward: 0.22003220647922905. Std of Reward: 0.9044508080557713.\n",
      "Step: 6320000. Mean Reward: 0.19918621328317962. Std of Reward: 0.9390361934987621.\n",
      "Step: 6330000. Mean Reward: 0.32611393034586067. Std of Reward: 0.8818217888308431.\n",
      "Step: 6340000. Mean Reward: 0.5111767540396406. Std of Reward: 0.8757899271667423.\n",
      "Step: 6350000. Mean Reward: 0.5287631853550022. Std of Reward: 0.8648822633026536.\n",
      "Saved Model\n",
      "Step: 6360000. Mean Reward: 0.47225285472107614. Std of Reward: 0.844150024003581.\n",
      "Step: 6370000. Mean Reward: 0.5272512636339214. Std of Reward: 0.8553282018348234.\n",
      "Step: 6380000. Mean Reward: 0.5793834373113367. Std of Reward: 0.840580181605451.\n",
      "Step: 6390000. Mean Reward: 0.5231680969096928. Std of Reward: 0.8510026095826803.\n",
      "Step: 6400000. Mean Reward: 0.5241729811503427. Std of Reward: 0.8838058348992933.\n",
      "Saved Model\n",
      "Step: 6410000. Mean Reward: 0.3709596528892495. Std of Reward: 0.9286450359130483.\n",
      "Step: 6420000. Mean Reward: 0.3507515785102224. Std of Reward: 0.9230425460418746.\n",
      "Step: 6430000. Mean Reward: 0.35738524448493825. Std of Reward: 0.9486926338979109.\n",
      "Step: 6440000. Mean Reward: 0.3274712049864215. Std of Reward: 0.9549012775856631.\n",
      "Step: 6450000. Mean Reward: 0.2196302048054819. Std of Reward: 0.9394420230802402.\n",
      "Saved Model\n",
      "Step: 6460000. Mean Reward: 0.2268610558469431. Std of Reward: 0.9803975261447683.\n",
      "Step: 6470000. Mean Reward: 0.21295786581358914. Std of Reward: 0.9448129317762368.\n",
      "Step: 6480000. Mean Reward: 0.17407212082987475. Std of Reward: 0.978774577138552.\n",
      "Step: 6490000. Mean Reward: 0.14309365667959542. Std of Reward: 0.9541176155226199.\n",
      "Step: 6500000. Mean Reward: 0.19157111876243332. Std of Reward: 0.9535035407107635.\n",
      "Saved Model\n",
      "Step: 6510000. Mean Reward: 0.17533610553000786. Std of Reward: 0.9660839488898503.\n",
      "Step: 6520000. Mean Reward: 0.14043886098250044. Std of Reward: 0.974724708230992.\n",
      "Step: 6530000. Mean Reward: 0.1523831971804466. Std of Reward: 0.9357379224154723.\n",
      "Step: 6540000. Mean Reward: 0.20251280679481626. Std of Reward: 0.94143015871989.\n",
      "Step: 6550000. Mean Reward: 0.1958552455683513. Std of Reward: 0.9038458313457128.\n",
      "Saved Model\n",
      "Step: 6560000. Mean Reward: 0.21763628359795456. Std of Reward: 0.9075648557943932.\n",
      "Step: 6570000. Mean Reward: 0.2234802823467622. Std of Reward: 0.9122065911997922.\n",
      "Step: 6580000. Mean Reward: 0.2602623153442318. Std of Reward: 0.9257060005337753.\n",
      "Step: 6590000. Mean Reward: 0.23306105203082378. Std of Reward: 0.964243085304872.\n",
      "Step: 6600000. Mean Reward: 0.20259809546240665. Std of Reward: 0.9506915150894922.\n",
      "Saved Model\n",
      "Step: 6610000. Mean Reward: 0.24455565681524838. Std of Reward: 0.9428098263010327.\n",
      "Step: 6620000. Mean Reward: 0.25194068903739175. Std of Reward: 0.9378736749287644.\n",
      "Step: 6630000. Mean Reward: 0.2262861893490243. Std of Reward: 0.9721679926847031.\n",
      "Step: 6640000. Mean Reward: 0.25473037629632406. Std of Reward: 0.9493816455474822.\n",
      "Step: 6650000. Mean Reward: 0.24094644103013182. Std of Reward: 0.9425959095135633.\n",
      "Saved Model\n",
      "Step: 6660000. Mean Reward: 0.23627303910420552. Std of Reward: 0.8915770483631406.\n",
      "Step: 6670000. Mean Reward: 0.21335633761217587. Std of Reward: 0.869204035247996.\n",
      "Step: 6680000. Mean Reward: 0.21370694886348854. Std of Reward: 0.8939340901621773.\n",
      "Step: 6690000. Mean Reward: 0.20765801010885765. Std of Reward: 0.8888693875225805.\n",
      "Step: 6700000. Mean Reward: 0.21364274910326603. Std of Reward: 0.8717984925821801.\n",
      "Saved Model\n",
      "Step: 6710000. Mean Reward: 0.2920871827193277. Std of Reward: 0.9583598607187201.\n",
      "Step: 6720000. Mean Reward: 0.24686145105646434. Std of Reward: 0.9615586861985025.\n",
      "Step: 6730000. Mean Reward: 0.23360301836769645. Std of Reward: 0.9635225704782624.\n",
      "Step: 6740000. Mean Reward: 0.2526630219492231. Std of Reward: 0.9359867458604677.\n",
      "Step: 6750000. Mean Reward: 0.27765631476002794. Std of Reward: 0.948142946304751.\n",
      "Saved Model\n",
      "Step: 6760000. Mean Reward: 0.25116282480107854. Std of Reward: 0.9710255748078486.\n",
      "Step: 6770000. Mean Reward: 0.2659599084042785. Std of Reward: 0.9836914352857887.\n",
      "Step: 6780000. Mean Reward: 0.27932698478390855. Std of Reward: 0.9540198230797335.\n",
      "Step: 6790000. Mean Reward: 0.2766258774792149. Std of Reward: 0.9798413544004406.\n",
      "Step: 6800000. Mean Reward: 0.285395579817022. Std of Reward: 0.9551718644400874.\n",
      "Saved Model\n",
      "Step: 6810000. Mean Reward: 0.25453429516116904. Std of Reward: 0.9455042621527743.\n",
      "Step: 6820000. Mean Reward: 0.26200304741461633. Std of Reward: 0.9559253790499257.\n",
      "Step: 6830000. Mean Reward: 0.2599533402657182. Std of Reward: 0.928972033602572.\n",
      "Step: 6840000. Mean Reward: 0.28794692345290734. Std of Reward: 0.9522873602483461.\n",
      "Step: 6850000. Mean Reward: 0.2857153987549186. Std of Reward: 0.9445207570034065.\n",
      "Saved Model\n",
      "Step: 6860000. Mean Reward: 0.27082625872382066. Std of Reward: 0.9561226099497554.\n",
      "Step: 6870000. Mean Reward: 0.2286223828313415. Std of Reward: 0.973695123179794.\n",
      "Step: 6880000. Mean Reward: 0.2777049073551086. Std of Reward: 0.9415211387599234.\n",
      "Step: 6890000. Mean Reward: 0.22505135539923204. Std of Reward: 0.972780541390066.\n",
      "Step: 6900000. Mean Reward: 0.22482627385091578. Std of Reward: 1.0019510155807003.\n",
      "Saved Model\n",
      "Step: 6910000. Mean Reward: 0.2760354676279487. Std of Reward: 0.956215865550269.\n",
      "Step: 6920000. Mean Reward: 0.27697907593255616. Std of Reward: 0.9631970998878494.\n",
      "Step: 6930000. Mean Reward: 0.22967738466996063. Std of Reward: 0.9533182795648365.\n",
      "Step: 6940000. Mean Reward: 0.24292650339444732. Std of Reward: 0.9564543778873531.\n",
      "Step: 6950000. Mean Reward: 0.22651473504088346. Std of Reward: 0.9307189086216157.\n",
      "Saved Model\n",
      "Step: 6960000. Mean Reward: 0.2452397451543344. Std of Reward: 0.9661461423793047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6970000. Mean Reward: 0.25694827798238534. Std of Reward: 0.9339327526308899.\n",
      "Step: 6980000. Mean Reward: 0.23759862036470683. Std of Reward: 0.9270328024054929.\n",
      "Step: 6990000. Mean Reward: 0.21157020800540924. Std of Reward: 0.9540913019422116.\n",
      "Step: 7000000. Mean Reward: 0.2338967615072466. Std of Reward: 0.9455585161094274.\n",
      "Saved Model\n",
      "Step: 7010000. Mean Reward: 0.2301190520389679. Std of Reward: 0.9415788809772445.\n",
      "Step: 7020000. Mean Reward: 0.2325166652082551. Std of Reward: 0.9465079824768358.\n",
      "Step: 7030000. Mean Reward: 0.24705904781980437. Std of Reward: 0.9658506771340248.\n",
      "Step: 7040000. Mean Reward: 0.20232636720034208. Std of Reward: 0.9528648135952799.\n",
      "Step: 7050000. Mean Reward: 0.23612785897434138. Std of Reward: 0.9775233304196529.\n",
      "Saved Model\n",
      "Step: 7060000. Mean Reward: 0.2232585904783533. Std of Reward: 0.9566761125638432.\n",
      "Step: 7070000. Mean Reward: 0.21230152568916097. Std of Reward: 0.9524605013076778.\n",
      "Step: 7080000. Mean Reward: 0.19031903015591423. Std of Reward: 0.9241842373473188.\n",
      "Step: 7090000. Mean Reward: 0.22494064023467827. Std of Reward: 0.9604432937792836.\n",
      "Step: 7100000. Mean Reward: 0.21636553292794647. Std of Reward: 0.9410699375354583.\n",
      "Saved Model\n",
      "Step: 7110000. Mean Reward: 0.16458282307025818. Std of Reward: 0.9596762779760487.\n",
      "Step: 7120000. Mean Reward: 0.23624411272435483. Std of Reward: 0.9524912321508165.\n",
      "Step: 7130000. Mean Reward: 0.2424517460639914. Std of Reward: 0.9342146372554331.\n",
      "Step: 7140000. Mean Reward: 0.26163196424259133. Std of Reward: 0.9734710601946982.\n",
      "Step: 7150000. Mean Reward: 0.2816284874764741. Std of Reward: 0.9340000425761568.\n",
      "Saved Model\n",
      "Step: 7160000. Mean Reward: 0.2512305300583661. Std of Reward: 0.9647398676834712.\n",
      "Step: 7170000. Mean Reward: 0.20772347995194562. Std of Reward: 0.9550819818015456.\n",
      "Step: 7180000. Mean Reward: 0.24477499840626593. Std of Reward: 0.9545862491457984.\n",
      "Step: 7190000. Mean Reward: 0.24112783699273588. Std of Reward: 0.9459161329765844.\n",
      "Step: 7200000. Mean Reward: 0.26715044880282457. Std of Reward: 0.9484768464710838.\n",
      "Saved Model\n",
      "Step: 7210000. Mean Reward: 0.26870715904196674. Std of Reward: 0.9608712702949321.\n",
      "Step: 7220000. Mean Reward: 0.24456827607626205. Std of Reward: 0.933748235358075.\n",
      "Step: 7230000. Mean Reward: 0.2389497370799273. Std of Reward: 0.9514423317379175.\n",
      "Step: 7240000. Mean Reward: 0.2686857385938018. Std of Reward: 0.9488608485966198.\n",
      "Step: 7250000. Mean Reward: 0.29288976704543557. Std of Reward: 0.9469569238106721.\n",
      "Saved Model\n",
      "Step: 7260000. Mean Reward: 0.23342668824896276. Std of Reward: 0.9505256248549648.\n",
      "Step: 7270000. Mean Reward: 0.23032788329299136. Std of Reward: 0.9419182510715168.\n",
      "Step: 7280000. Mean Reward: 0.2494615911571119. Std of Reward: 0.962479922852715.\n",
      "Step: 7290000. Mean Reward: 0.2527696463933946. Std of Reward: 0.9546986930792336.\n",
      "Step: 7300000. Mean Reward: 0.2419649070722208. Std of Reward: 0.9665843230188146.\n",
      "Saved Model\n",
      "Step: 7310000. Mean Reward: 0.23867613250803446. Std of Reward: 0.9559663743672048.\n",
      "Step: 7320000. Mean Reward: 0.23697159261859183. Std of Reward: 0.9752298993931883.\n",
      "Step: 7330000. Mean Reward: 0.21285309253649914. Std of Reward: 0.9474704351259885.\n",
      "Step: 7340000. Mean Reward: 0.25648012532368636. Std of Reward: 0.9692798057548142.\n",
      "Step: 7350000. Mean Reward: 0.2769832882033511. Std of Reward: 0.9503032850772821.\n",
      "Saved Model\n",
      "Step: 7360000. Mean Reward: 0.2058562540872345. Std of Reward: 0.9580181761881053.\n",
      "Step: 7370000. Mean Reward: 0.2277981587231262. Std of Reward: 0.96900977552533.\n",
      "Step: 7380000. Mean Reward: 0.24775821160989706. Std of Reward: 0.9646885956173998.\n",
      "Step: 7390000. Mean Reward: 0.2789970905652397. Std of Reward: 0.9472548201264556.\n",
      "Step: 7400000. Mean Reward: 0.24551021909700294. Std of Reward: 0.9644205188056794.\n",
      "Saved Model\n",
      "Step: 7410000. Mean Reward: 0.2380192915704775. Std of Reward: 0.9500264511324286.\n",
      "Step: 7420000. Mean Reward: 0.26317922155293394. Std of Reward: 0.9640677102497429.\n",
      "Step: 7430000. Mean Reward: 0.26815751268984755. Std of Reward: 0.9812892394720854.\n",
      "Step: 7440000. Mean Reward: 0.20819785849904332. Std of Reward: 0.9585064572855713.\n",
      "Step: 7450000. Mean Reward: 0.22904256440806642. Std of Reward: 0.9861978339141012.\n",
      "Saved Model\n",
      "Step: 7460000. Mean Reward: 0.24761465162527455. Std of Reward: 0.9227957632434427.\n",
      "Step: 7470000. Mean Reward: 0.21561382113789243. Std of Reward: 0.9710361749589576.\n",
      "Step: 7480000. Mean Reward: 0.18026832391746. Std of Reward: 0.9390038774944413.\n",
      "Step: 7490000. Mean Reward: 0.2523601475129932. Std of Reward: 0.9590315811786614.\n",
      "Step: 7500000. Mean Reward: 0.22022707315189635. Std of Reward: 0.9530841112275621.\n",
      "Saved Model\n",
      "Step: 7510000. Mean Reward: 0.2504293794104512. Std of Reward: 0.9795979244528727.\n",
      "Step: 7520000. Mean Reward: 0.21960233170001092. Std of Reward: 0.9526913055831215.\n",
      "Step: 7530000. Mean Reward: 0.26848281743857094. Std of Reward: 0.9478633110864993.\n",
      "Step: 7540000. Mean Reward: 0.21934693066156305. Std of Reward: 0.9384373889306504.\n",
      "Step: 7550000. Mean Reward: 0.23805377954944482. Std of Reward: 0.9320527701596926.\n",
      "Saved Model\n",
      "Step: 7560000. Mean Reward: 0.2149076730006191. Std of Reward: 0.946910355745215.\n",
      "Step: 7570000. Mean Reward: 0.2520410114494754. Std of Reward: 0.9375945071251812.\n",
      "Step: 7580000. Mean Reward: 0.2644720468115032. Std of Reward: 0.9376376124720093.\n",
      "Step: 7590000. Mean Reward: 0.24000490698519447. Std of Reward: 0.9233045918824053.\n",
      "Step: 7600000. Mean Reward: 0.27174614224946936. Std of Reward: 0.9076943805282065.\n",
      "Saved Model\n",
      "Step: 7610000. Mean Reward: 0.23376291296251492. Std of Reward: 0.884854297364287.\n",
      "Step: 7620000. Mean Reward: 0.1780740234136588. Std of Reward: 0.8794028720174053.\n",
      "Step: 7630000. Mean Reward: 0.18040455053502225. Std of Reward: 0.8719772364842961.\n",
      "Step: 7640000. Mean Reward: 0.18646666476642035. Std of Reward: 0.8924924057316262.\n",
      "Step: 7650000. Mean Reward: 0.16290996238647776. Std of Reward: 0.8587135085121806.\n",
      "Saved Model\n",
      "Step: 7660000. Mean Reward: 0.1963982781312195. Std of Reward: 0.8835084364145337.\n",
      "Step: 7670000. Mean Reward: 0.14965401220075472. Std of Reward: 0.8554767409772756.\n",
      "Step: 7680000. Mean Reward: 0.18318303481857537. Std of Reward: 0.8696104860316466.\n",
      "Step: 7690000. Mean Reward: 0.14425565181624364. Std of Reward: 0.8281304880278062.\n",
      "Step: 7700000. Mean Reward: 0.15108161841573334. Std of Reward: 0.8274469062270051.\n",
      "Saved Model\n",
      "Step: 7710000. Mean Reward: 0.10883567940785387. Std of Reward: 0.8303368213172946.\n",
      "Step: 7720000. Mean Reward: 0.1445062409396016. Std of Reward: 0.8512230123870325.\n",
      "Step: 7730000. Mean Reward: 0.12537838717114733. Std of Reward: 0.8711266194143287.\n",
      "Step: 7740000. Mean Reward: 0.22003352156812883. Std of Reward: 0.8943512436185667.\n",
      "Step: 7750000. Mean Reward: 0.19526544379315333. Std of Reward: 0.8673334361446049.\n",
      "Saved Model\n",
      "Step: 7760000. Mean Reward: 0.16003319437942826. Std of Reward: 0.8463784822009139.\n",
      "Step: 7770000. Mean Reward: 0.10031798996225068. Std of Reward: 0.8380662784529921.\n",
      "Step: 7780000. Mean Reward: 0.13342368570611315. Std of Reward: 0.8308286890017391.\n",
      "Step: 7790000. Mean Reward: 0.12440652807766202. Std of Reward: 0.8210741627694051.\n",
      "Step: 7800000. Mean Reward: 0.1744138338835071. Std of Reward: 0.8256430841923372.\n",
      "Saved Model\n",
      "Step: 7810000. Mean Reward: 0.15098670015034202. Std of Reward: 0.8478522133760559.\n",
      "Step: 7820000. Mean Reward: 0.13757640288266285. Std of Reward: 0.8378517689088676.\n",
      "Step: 7830000. Mean Reward: 0.07757834301643617. Std of Reward: 0.8014757330679234.\n",
      "Step: 7840000. Mean Reward: 0.10667391941494406. Std of Reward: 0.8039237092606352.\n",
      "Step: 7850000. Mean Reward: 0.08295444035185791. Std of Reward: 0.7984213147515284.\n",
      "Saved Model\n",
      "Step: 7860000. Mean Reward: 0.03751360036401369. Std of Reward: 0.8164483379034848.\n",
      "Step: 7870000. Mean Reward: 0.03923680458551228. Std of Reward: 0.789981600250462.\n",
      "Step: 7880000. Mean Reward: 0.07138889094195024. Std of Reward: 0.7863013481838039.\n",
      "Step: 7890000. Mean Reward: 0.06297200879604638. Std of Reward: 0.790345893441857.\n",
      "Step: 7900000. Mean Reward: 0.04674717306387813. Std of Reward: 0.7813123322808089.\n",
      "Saved Model\n",
      "Step: 7910000. Mean Reward: 0.007455133742123286. Std of Reward: 0.7554861462812803.\n",
      "Step: 7920000. Mean Reward: 0.012018237148456094. Std of Reward: 0.772703957317845.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7930000. Mean Reward: 0.031050327329081848. Std of Reward: 0.8193390349661247.\n",
      "Step: 7940000. Mean Reward: 0.02915615835675506. Std of Reward: 0.7700987151993515.\n",
      "Step: 7950000. Mean Reward: 0.08198024549122733. Std of Reward: 0.7947092102908042.\n",
      "Saved Model\n",
      "Step: 7960000. Mean Reward: 0.06611764470634617. Std of Reward: 0.768662120165768.\n",
      "Step: 7970000. Mean Reward: 0.08216241866872113. Std of Reward: 0.8044712022920306.\n",
      "Step: 7980000. Mean Reward: 0.05554139721007139. Std of Reward: 0.7664065754503651.\n",
      "Step: 7990000. Mean Reward: 0.06585490164039702. Std of Reward: 0.8169401797543675.\n",
      "Step: 8000000. Mean Reward: 0.04770186709286286. Std of Reward: 0.7999964362931478.\n",
      "Saved Model\n",
      "Step: 8010000. Mean Reward: 0.09745105879776081. Std of Reward: 0.8394342443423812.\n",
      "Step: 8020000. Mean Reward: 0.06430711017858401. Std of Reward: 0.7950951229549679.\n",
      "Step: 8030000. Mean Reward: 0.1157247781481688. Std of Reward: 0.8521563246188882.\n",
      "Step: 8040000. Mean Reward: 0.10541046292819212. Std of Reward: 0.7976299491763539.\n",
      "Step: 8050000. Mean Reward: 0.09081565096511524. Std of Reward: 0.8028712524953818.\n",
      "Saved Model\n",
      "Step: 8060000. Mean Reward: 0.0673565827562011. Std of Reward: 0.8016012096213118.\n",
      "Step: 8070000. Mean Reward: 0.06469591218062812. Std of Reward: 0.7929302000910734.\n",
      "Step: 8080000. Mean Reward: 0.08612857139850882. Std of Reward: 0.8049112187075339.\n",
      "Step: 8090000. Mean Reward: 0.06116306878245591. Std of Reward: 0.7920030309926017.\n",
      "Step: 8100000. Mean Reward: 0.036248046169339784. Std of Reward: 0.7818468462910826.\n",
      "Saved Model\n",
      "Step: 8110000. Mean Reward: 0.10224491468590569. Std of Reward: 0.8054419017887817.\n",
      "Step: 8120000. Mean Reward: 0.06563759845253746. Std of Reward: 0.8184323922501526.\n",
      "Step: 8130000. Mean Reward: 0.1067513465141266. Std of Reward: 0.8060343616460918.\n",
      "Step: 8140000. Mean Reward: 0.09204900446129988. Std of Reward: 0.8023393789297756.\n",
      "Step: 8150000. Mean Reward: 0.08623939381570213. Std of Reward: 0.7636724013232548.\n",
      "Saved Model\n",
      "Step: 8160000. Mean Reward: 0.07859930963257897. Std of Reward: 0.8011331461996396.\n",
      "Step: 8170000. Mean Reward: 0.10291821114231911. Std of Reward: 0.8134735542024464.\n",
      "Step: 8180000. Mean Reward: 0.06659018325865168. Std of Reward: 0.8145621436801216.\n",
      "Step: 8190000. Mean Reward: 0.0755229838456473. Std of Reward: 0.8129193006781649.\n",
      "Step: 8200000. Mean Reward: 0.09373330752902176. Std of Reward: 0.8416580514696546.\n",
      "Saved Model\n",
      "Step: 8210000. Mean Reward: 0.07353497876369239. Std of Reward: 0.789629957726518.\n",
      "Step: 8220000. Mean Reward: 0.08083588391655065. Std of Reward: 0.7986843139902434.\n",
      "Step: 8230000. Mean Reward: 0.061501723169325034. Std of Reward: 0.8073320698971745.\n",
      "Step: 8240000. Mean Reward: 0.05636504365896569. Std of Reward: 0.8049858590866723.\n",
      "Step: 8250000. Mean Reward: 0.05183866957929683. Std of Reward: 0.8042055448533807.\n",
      "Saved Model\n",
      "Step: 8260000. Mean Reward: 0.035228090176700796. Std of Reward: 0.7792084224934823.\n",
      "Step: 8270000. Mean Reward: 0.05133665081698207. Std of Reward: 0.7776705940000667.\n",
      "Step: 8280000. Mean Reward: 0.04264286274372652. Std of Reward: 0.811042545830546.\n",
      "Step: 8290000. Mean Reward: 0.061296032417591524. Std of Reward: 0.8133289762331762.\n",
      "Step: 8300000. Mean Reward: 0.029430868598785537. Std of Reward: 0.7898319737854504.\n",
      "Saved Model\n",
      "Step: 8310000. Mean Reward: 0.06117913470931099. Std of Reward: 0.7861008811263057.\n",
      "Step: 8320000. Mean Reward: 0.08678081980772348. Std of Reward: 0.8054234276686782.\n",
      "Step: 8330000. Mean Reward: 0.05450628336454193. Std of Reward: 0.8038975469711568.\n",
      "Step: 8340000. Mean Reward: 0.08232114286004938. Std of Reward: 0.8050597528577359.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-50001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo/model-50001.cptk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0d967e19d86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexport_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/ml-agents/python/ppo/models.py\u001b[0m in \u001b[0;36mexport_graph\u001b[0;34m(model_path, env_name, target_nodes)\u001b[0m\n\u001b[1;32m     56\u001b[0m                               \u001b[0moutput_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.bytes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                               \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_saver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                               restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\")\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph\u001b[0;34m(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_whitelist, variable_names_blacklist, input_meta_graph, input_saved_model_dir, saved_model_tags)\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mrestore_op_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names_whitelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names_blacklist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       input_meta_graph_def, input_saved_model_dir, saved_model_tags.split(\",\"))\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/tools/freeze_graph.py\u001b[0m in \u001b[0;36mfreeze_graph_with_def_protos\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mvar_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1666\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 run_metadata):\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1354\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1041\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1042\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/encoder.py\u001b[0m in \u001b[0;36mEncodeRepeatedField\u001b[0;34m(write, value, deterministic)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEncodeRepeatedField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value, deterministic)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0mvalue_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m       \u001b[0mentry_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m       \u001b[0mencode_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
